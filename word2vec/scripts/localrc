# constants
timestamp=`date +%Y%m%d%H%M%S`
# variables
## Use text data from <file> to train the model
FILEIN=news.raw.20171104
#FILEIN=zhwiki-latest-pages-articles.1020.seg
## Set size of word vectors; default is 100
EMBEDDING_DIM=100
## Set max skip length between words; default is 5
WINDOW=5
## Set threshold for occurrence of words. Those that appear with higher frequency in the training data
## will be randomly down-sampled; default is 1e-3, useful range is (0, 1e-5)
SAMPLE=1e-3
## Number of negative examples; default is 5, common values are 3 - 10 (0 = not used)
NEGATIVE=5
## Use Hierarchical Softmax; default is 0 (not used)
HS=0
## Use the continuous bag of words model; default is 1 (use 0 for skip-gram model)
CBOW=1
## Run more training iterations (default 5)
ITER=200
## Save the resulting vectors in binary moded; default is 0 (off)
BINARY=1
## This will discard words that appear less than <int> times; default is 5
MINCOUNT=10
## Use <int> threads
THREADS=1
## Save vocab
VOCAB=news.vocab
## Use <file> to save the resulting word vectors / word clusters
FILEOUT=news.min$MINCOUNT.neg$NEGATIVE.cbow$CBOW.win$WINDOW.iter$ITER.embed$EMBEDDING_DIM.thr$THREADS.bin.$timestamp
